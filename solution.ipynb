import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, recall_score
from nltk import ngrams
from collections import Counter

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

data = pd.read_csv("data/train.csv")
#Task A
disasterTweets = data[data['sentiment'] == 'negative']['review']
normalTweets = data[data['sentiment'] == 'positive']['review']

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    tokens = word_tokenize(text)
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return ' '.join(lemmatized_tokens)

vectorizer = CountVectorizer()

disaster_word_counts = vectorizer.fit_transform(disasterTweets)
normal_word_counts = vectorizer.transform(normalTweets)

disaster_word_counts = np.array(disaster_word_counts.sum(axis=0)).flatten()
normal_word_counts = np.array(normal_word_counts.sum(axis=0)).flatten()

disaster_top_words = pd.Series(disaster_word_counts, index=vectorizer.get_feature_names_out())
normal_top_words = pd.Series(normal_word_counts, index=vectorizer.get_feature_names_out())

print("Top 20 unique words in disaster tweets:")
print(disaster_top_words.sort_values(ascending=False).head(20))

print("\nTop 20 unique words in normal tweets:")
print(normal_top_words.sort_values(ascending=False).head(20))

def get_ngrams(text, n):
    n_grams = list(ngrams(text.split(), n))
    return n_grams
disaster_bigrams = disasterTweets.apply(lambda x: get_ngrams(x, 2)).explode()
disaster_trigrams = disasterTweets.apply(lambda x: get_ngrams(x, 3)).explode()

normal_bigrams = normalTweets.apply(lambda x: get_ngrams(x, 2)).explode()
normal_trigrams = normalTweets.apply(lambda x: get_ngrams(x, 3)).explode()

print("\nTop 20 bigrams in disaster tweets:")
print(disaster_bigrams.value_counts().head(20))

print("\nTop 20 trigrams in disaster tweets:")
print(disaster_trigrams.value_counts().head(20))

print("\nTop 20 bigrams in normal tweets:")
print(normal_bigrams.value_counts().head(20))

print("\nTop 20 trigrams in normal tweets:")
print(normal_trigrams.value_counts().head(20))

#Task B
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])
    return text

data['changed_text'] = data['review'].apply(preprocess_text)

all_tokens = [token for text in data['changed_text'] for token in word_tokenize(text)]
token_counts = Counter(all_tokens)

print("Total token count for Changed token:", len(all_tokens))
print("Unique token count for Changed token: ", len(token_counts))
print("\nToken counts:")
for token, count in token_counts.items():
    print(f"{token}: {count}")

all_tokens_review = [token for text in data['review'] for token in word_tokenize(text)]
token_counts_review = Counter(all_tokens_review)

print("Total token count for 'review':", len(all_tokens_review))
print("Unique token count for 'review':", len(token_counts_review))
print("\nToken counts for 'review':")
for token, count in token_counts_review.items():
    print(f"{token}: {count}")

for index, row in data.iterrows():
    if row['changed_text'] != row['review']:
        print(f"Row {index}:")
        print(f"Changed Text: {row['changed_text']}")
        print(f"Original Review: {row['review']}")
        print("\n")
#Task C
X_train, X_test, y_train, y_test = train_test_split(data['changed_text'], data['sentiment'], test_size=0.2, random_state=50)
test_data=pd.read_csv('data/test.csv')

def evaluate_model(X_train, X_test, y_train, y_test, max_features):
    vectorizer = CountVectorizer(max_features=max_features)
    X_train_vectorized = vectorizer.fit_transform(X_train)
    X_test_vectorized = vectorizer.transform(X_test)

    model = LogisticRegression()
    model.fit(X_train_vectorized, y_train)
    y_pred = model.predict(X_test_vectorized)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred,pos_label='positive')
    recall = recall_score(y_test, y_pred,pos_label='positive')

    return accuracy, f1, recall, model, vectorizer

max_features_list = [100, 1000]

for max_features in max_features_list:
    accuracy, f1, recall, trained_model, trained_vectorizer = evaluate_model(X_train, X_test, y_train, y_test, max_features)
    print(f"\nResults for max_features={max_features}:")
    print(f"Accuracy: {accuracy}")
    print(f"F1 Score: {f1}")
    print(f"Recall: {recall}")

    # Preprocess test data
    test_data['clean_text'] = test_data['review'].apply(preprocess_text)

    # Vectorize test data using trained CountVectorizer
    X_test_vectorized = trained_vectorizer.transform(test_data['clean_text'])

    # Make predictions on test data using trained model
    y_pred_test = trained_model.predict(X_test_vectorized)

    # Create DataFrame with predictions and IDs
    results = pd.DataFrame({'id': test_data['id'], 'sentiment': y_pred_test})

    # Print results
    print(results)
